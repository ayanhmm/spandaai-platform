{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d0ba7-4082-4390-8a18-e19677915ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket_name_in_minio = \"mhmm\"\n",
    "minio_ip = \"172.18.0.4\"\n",
    "kafka_ip = \"172.18.0.6\"\n",
    "\n",
    "sensor_topic = \"temp_producer\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1fb670-198f-4634-8b72-472822cead0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97af7937-77df-4e81-be72-6ead3be2d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\" ## Nessie Server URI\n",
    "WAREHOUSE = \"s3://\" + data_bucket_name_in_minio +\"/\" ## S3 Address to Write to\n",
    "STORAGE_URI = \"http://\"+ minio_ip +\":9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53705657-dc6b-4c76-bf22-eb379a5936ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"KafkaSparkIceberg\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \",\".join([\n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n",
    "                \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0\",\n",
    "                \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1\",\n",
    "                \"software.amazon.awssdk:bundle:2.24.8\",\n",
    "                \"software.amazon.awssdk:url-connection-client:2.24.8\"\n",
    "            ]))\n",
    "    .config(\"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,\"\n",
    "            \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .config(\"spark.sql.catalog.nessie.uri\", CATALOG_URI)\n",
    "    .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .config(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "    .config(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "    .config(\"spark.sql.catalog.nessie.s3.endpoint\", STORAGE_URI)\n",
    "    .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"âœ… Spark Kafka-Nessie setup ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284be111-1d4e-48b8-b6fe-6306f16c9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"source\", StringType()) \\\n",
    "    .add(\"temp\", DoubleType()) \\\n",
    "    .add(\"timestamp\", TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679ca8d8-6be0-4094-a5d5-e4e4511bd026",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_in_nessie = sensor_topic + \"_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696c92c-7240-47b6-ba36-9246a88e1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.\" + table_name_in_nessie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794239f-7f75-4a56-a983-2286f0a98ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS nessie.\" + table_name_in_nessie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a03b5-c5e0-4dc5-a2d9-7868211527d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"check 1\")\n",
    "# Read data from Kafka topic\n",
    "kafka_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_ip + \":9092\")\n",
    "    .option(\"subscribe\", sensor_topic)\n",
    "    .option(\"startingOffsets\", \"earliest\")  # or \"earliest/latest\"\n",
    "    # .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert the binary Kafka `value` column into structured JSON\n",
    "parsed_df = (\n",
    "    kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "# Optional transformation (filter, aggregate, etc.)\n",
    "processed_df = parsed_df.filter(col(\"temp\") > 0)\n",
    "\n",
    "print(\"check 2\")\n",
    "# Write stream to console for testing\n",
    "query = (\n",
    "    parsed_df.writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/workspace/tables\")  # necessary\n",
    "    .trigger(processingTime=\"60 seconds\") \n",
    "    .toTable(\"nessie.\" + table_name_in_nessie)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(\"check 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6adcdbd-cd0f-4658-9888-f211520bf0aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spark.read.table(\"nessie.sensor_yo\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4ab25-e588-4472-8fd0-e9e0d8613904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"check 1\")\n",
    "# # Read data from Kafka topic\n",
    "# kafka_df = (\n",
    "#     spark.readStream\n",
    "#     .format(\"kafka\")\n",
    "#     .option(\"kafka.bootstrap.servers\", kafka_ip + \":9092\")\n",
    "#     .option(\"subscribe\", sensor_topic)\n",
    "#     .option(\"startingOffsets\", \"earliest\")  # or \"earliest/latest\"\n",
    "#     # .option(\"failOnDataLoss\", \"false\")\n",
    "#     .load()\n",
    "# )\n",
    "\n",
    "# # Convert the binary Kafka `value` column into structured JSON\n",
    "# parsed_df = (\n",
    "#     kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "#     .select(from_json(col(\"value\"), schema).alias(\"data\"))\n",
    "#     .select(\"data.*\")\n",
    "# )\n",
    "\n",
    "# # Optional transformation (filter, aggregate, etc.)\n",
    "# processed_df = parsed_df.filter(col(\"temp\") > 0)\n",
    "\n",
    "# print(\"check 2\")\n",
    "# # Write stream to console for testing\n",
    "# query = (\n",
    "#     processed_df.writeStream\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"append\")\n",
    "#     .option(\"truncate\", False)\n",
    "#     .start()\n",
    "# )\n",
    "\n",
    "# print(\"check 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebf6bf77-8b5c-49fd-b046-8a3cce0618f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
