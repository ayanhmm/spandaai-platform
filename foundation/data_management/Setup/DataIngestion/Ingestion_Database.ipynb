{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6ed76-06e7-474f-9d5a-1c9e040f3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitive variables\n",
    "postgres_container_database_name = 'mydb'\n",
    "\n",
    "minio_ip = \"172.18.0.4\"\n",
    "# docker inspect minio | grep IPAddress\n",
    "# run above command to get ip at minio\n",
    "data_bucket_name_in_minio = \"mybucket\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca7cc4",
   "metadata": {},
   "source": [
    "# Uploading all tables from a Folder to a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a6798-b556-48d2-a301-ca32daaa2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the CSV to the containers\n",
    "# docker cp tables spark:/workspace/tables\n",
    "tables_folder_spark = '/workspace/tables'\n",
    "# docker cp tables postgres:/tables\n",
    "tables_folder_postgres = '/tables'\n",
    "\n",
    "# check csv folder properly imported to postgres\n",
    "import os\n",
    "tables_folder = tables_folder_spark\n",
    "files = os.listdir(tables_folder)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ae243-3042-46b9-b8d3-712810136998",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2-binary\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------Establishing a connection to the db--------------------------------\n",
    "# Set PostgreSQL connection parameters\n",
    "db_config = {\n",
    "    'host': 'postgres',         \n",
    "    'port': 5432,               # Default PostgreSQL port\n",
    "    'dbname': postgres_container_database_name,     \n",
    "    'user': 'myuser',           \n",
    "    'password': 'mypassword'    \n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    print(\"‚úÖ Connection successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"connection failed ‚ùå Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a5ba1-62e9-4c68-8d99-34733ef3c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_tables(conn):\n",
    "    if 'conn' in globals() and conn is not None:\n",
    "        confirm = input(\"‚ö†Ô∏è WARNING: This will delete ALL tables!\")\n",
    "        if confirm == \"Y\":\n",
    "            try:\n",
    "                # Get all tables\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(\"\"\"\n",
    "                    SELECT table_name\n",
    "                    FROM information_schema.tables\n",
    "                    WHERE table_schema = 'public'\n",
    "                    AND table_type = 'BASE TABLE';\n",
    "                \"\"\")\n",
    "                tables = cursor.fetchall()\n",
    "                \n",
    "                if not tables:\n",
    "                    print(\"No tables to delete.\")\n",
    "                else:\n",
    "                    # Disable foreign key checks (if needed)\n",
    "                    cursor.execute(\"SET session_replication_role = 'replica';\")\n",
    "                    \n",
    "                    # Drop all tables\n",
    "                    for table in tables:\n",
    "                        table_name = table[0]\n",
    "                        cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "                        print(f\"Dropped table: {table_name}\")\n",
    "                    \n",
    "                    # Re-enable foreign key checks\n",
    "                    cursor.execute(\"SET session_replication_role = 'origin';\")\n",
    "                    \n",
    "                    print(f\"\\n‚úÖ Successfully dropped {len(tables)} tables.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(\"‚ùå Error deleting tables:\", e)\n",
    "            finally:\n",
    "                cursor.close()\n",
    "        else:\n",
    "            print(\"Table deletion cancelled.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No active database connection. Please run the connection code first.\")\n",
    "def list_database_tables(conn, schema='public'):\n",
    "    \"\"\"\n",
    "    List all tables in the specified database schema.\n",
    "    \n",
    "    Parameters:\n",
    "    - conn: Database connection object\n",
    "    - schema: Schema name (defaults to 'public')\n",
    "    \n",
    "    Returns:\n",
    "    - Pandas DataFrame containing table names, or None if no tables found\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = '{schema}' \n",
    "        AND table_type = 'BASE TABLE';\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No tables found in the specified schema.\")\n",
    "        return None\n",
    "    else:\n",
    "        return df\n",
    "# delete_all_tables(conn)\n",
    "list_database_tables(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa33984-25bf-4e55-b019-a602491c48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------Uploading all tables from a folder--------------------------------\n",
    "# function to Infer SQL data types\n",
    "def infer_sql_type(series):\n",
    "    if pd.api.types.is_integer_dtype(series):\n",
    "        return \"INTEGER\"\n",
    "    elif pd.api.types.is_float_dtype(series):\n",
    "        return \"FLOAT\"\n",
    "    elif pd.api.types.is_bool_dtype(series):\n",
    "        return \"BOOLEAN\"\n",
    "    elif pd.api.types.is_datetime64_any_dtype(series):\n",
    "        return \"TIMESTAMP\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "# function to Create query to table\n",
    "def generate_create_table_sql(csv_path_generate_create_table_sql, table_name_generate_create_table_sql=None):\n",
    "    df = pd.read_csv(csv_path_generate_create_table_sql)\n",
    "    if table_name_generate_create_table_sql is None:\n",
    "        table_name_generate_create_table_sql = os.path.splitext(os.path.basename(csv_path_generate_create_table_sql))[0].replace(\" \", \"_\")\n",
    "\n",
    "    create_stmt = f\"CREATE TABLE IF NOT EXISTS {table_name_generate_create_table_sql} (\\n\"\n",
    "    columns = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_name = col.strip().replace(\" \", \"_\")\n",
    "        col_type = infer_sql_type(df[col])\n",
    "        constraint = \"NOT NULL\" if not df[col].isnull().any() else \"\"\n",
    "        columns.append(f\"  {col_name} {col_type} {constraint}\".strip())\n",
    "\n",
    "    create_stmt += \",\\n\".join(columns) + \"\\n);\"\n",
    "    return create_stmt, table_name_generate_create_table_sql\n",
    "# function to create table from csv and copy data\n",
    "def create_table_and_copy_data(csv_path, csv_name):\n",
    "    create_sql, table_name = generate_create_table_sql(csv_path)\n",
    "    # print('create statement:', create_sql)\n",
    "    print('table name:', table_name)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        cur.execute(create_sql)\n",
    "        conn.commit()\n",
    "        print(f\"‚úÖ Table `{table_name}` created.\")\n",
    "\n",
    "        file_path_create_table_and_copy_data = folder_path_postgres + '/' + csv_name\n",
    "        copy_sql = f\"\"\"\n",
    "        COPY {table_name}\n",
    "        FROM '{file_path_create_table_and_copy_data}'\n",
    "        DELIMITER ','\n",
    "        CSV HEADER;\n",
    "        \"\"\"\n",
    "        cur.execute(copy_sql)\n",
    "        conn.commit()\n",
    "        print(f\"üì• Data copied from `{csv_path}` to `{table_name}`.\")\n",
    "\n",
    "        # Preview data\n",
    "        cur.execute(f\"SELECT * FROM {table_name} LIMIT 2;\")\n",
    "        rows = cur.fetchall()\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "            \n",
    "# the above code only assigns data types to table columns, you may also want to assign integrity counstraints yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c76003-4175-43ae-af0e-36ae40aa627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline for all CSV files in the directory\n",
    "folder_path_postgres = tables_folder_postgres\n",
    "for filename in os.listdir(tables_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(tables_folder, filename)\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract table name from filename (removes .csv extension)\n",
    "            table_name = os.path.splitext(filename)[0]\n",
    "            create_table_and_copy_data(csv_file_path, filename)\n",
    "            print(f\"‚úÖ Successfully processed {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {str(e)}\")\n",
    "conn.rollback()  # Reset the transaction state\n",
    "\n",
    "# check if tables are created properly\n",
    "list_database_tables(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d223a5-5f01-440d-baeb-43ebfce71046",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab7bc2",
   "metadata": {},
   "source": [
    "# Uploading a running Database to Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f4d155b-1560-496e-ad9f-70253b430cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3e9b4ff-bcd7-4f6b-8f31-1153dcdf48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_database_name = postgres_container_database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73d9452f-edb9-4dd6-863f-5da52251df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\" ## Nessie Server URI\n",
    "WAREHOUSE = \"s3://\" + data_bucket_name_in_minio +\"/\" ## S3 Address to Write to\n",
    "STORAGE_URI = \"http://\"+ minio_ip +\":9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ad42e-de46-419a-a8ce-30dd52e02cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "          #packages\n",
    "        .set('spark.jars.packages', 'org.postgresql:postgresql:42.7.3,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,software.amazon.awssdk:bundle:2.24.8,software.amazon.awssdk:url-connection-client:2.24.8')\n",
    "          #SQL Extensions\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "          #Configuring Catalog\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', CATALOG_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', STORAGE_URI)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    ")\n",
    "\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf1104-60ed-4f99-90a7-8dc56f47da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the JDBC URL for the Postgres database\n",
    "# JDBC => Java Database Connectivity URL => a kind of protocol\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/\" + postgres_database_name\n",
    "properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Get list of table names from Postgres using Spark\n",
    "tables_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"(SELECT table_name FROM information_schema.tables WHERE table_schema='public' AND table_type='BASE TABLE') AS table_list\",\n",
    "    properties=properties\n",
    ")\n",
    "table_names = [row['table_name'] for row in tables_df.collect()]\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2419f06-0cf7-4b52-b07d-987951b40ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name in table_names:\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "    \n",
    "    # Load table from Postgres\n",
    "    postgres_df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=properties)\n",
    "    \n",
    "    # Write to Iceberg via Nessie\n",
    "    postgres_df.writeTo(f\"nessie.{table_name}\").createOrReplace()\n",
    "    \n",
    "    # Show contents of Iceberg table\n",
    "    print(f\"Contents of table: {table_name}\")\n",
    "    spark.read.table(f\"nessie.{table_name}\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "684d456f-46f5-467c-bd87-cb8f9f374575",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
